{
 "nbformat": 4,
 "nbformat_minor": 4,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ¦ Klasyfikacja GatunkÃ³w PtakÃ³w â€” Transfer Learning\n\n**Autor:** Wygenerowano z pomocÄ… Claude (Anthropic)  \n**Framework:** PyTorch + torchvision  \n**Model bazowy:** EfficientNet-B0 (pretrenowany na ImageNet)\n\n## Spis treÅ›ci\n1. Wprowadzenie â€” czym jest Transfer Learning?\n2. Instalacja i import bibliotek\n3. Przygotowanie danych\n4. Augmentacja i transformacje\n5. Budowa modelu z Transfer Learningiem\n6. Faza 1: Feature Extraction\n7. Faza 2: Fine-Tuning\n8. Ewaluacja modelu\n9. Predykcja na nowych obrazach\n10. Podsumowanie"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Wprowadzenie â€” Czym jest Transfer Learning?\n\n### Problem\nTrening od zera wymaga ogromnych zbiorÃ³w danych, duÅ¼ej mocy obliczeniowej i czasu.\n\n### RozwiÄ…zanie\nTransfer learning przenosi wiedzÄ™ z modelu wytrenowanego na duÅ¼ym zbiorze (ImageNet â€” 1.2M obrazÃ³w, 1000 klas) do nowego zadania.\n\n### Analogia\nUczÄ…c siÄ™ rozpoznawaÄ‡ ptaki, nie zaczynasz od zera â€” juÅ¼ wiesz jak wyglÄ…dajÄ… ksztaÅ‚ty, kolory, tekstury. Transfer learning dziaÅ‚a tak samo!\n\n### Dwie fazy\n| Faza | Opis | Learning Rate |\n|------|-------|---------------|\n| **Feature Extraction** | ZamroÅ¼one warstwy bazowe, trenujemy nowy klasyfikator | 0.001 |\n| **Fine-Tuning** | OdmroÅ¼one warstwy, delikatne dostrajanie | 0.0001 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Instalacja i import bibliotek"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Instalacja (odkomentuj jeÅ›li potrzebujesz)\n# !pip install torch torchvision matplotlib pillow numpy"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import os, copy, time, json\nfrom pathlib import Path\nfrom collections import defaultdict\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\nfrom torchvision import datasets, transforms, models\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom PIL import Image\n\n%matplotlib inline\nplt.style.use('seaborn-v0_8-whitegrid')\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f'UrzÄ…dzenie: {device}')\nif device.type == 'cuda':\n    print(f'GPU: {torch.cuda.get_device_name(0)}')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Przygotowanie danych\n\nDane powinny mieÄ‡ strukturÄ™:\n```\ndata/\nâ”œâ”€â”€ train/  (70%)\nâ”‚   â”œâ”€â”€ American_Robin/\nâ”‚   â”œâ”€â”€ Blue_Jay/\nâ”‚   â””â”€â”€ ...\nâ”œâ”€â”€ val/    (15%)\nâ””â”€â”€ test/   (15%)\n```\n\n**Polecane datasety:**\n- [Birds 525 Species (Kaggle)](https://www.kaggle.com/datasets/gpiosenka/100-bird-species)\n- [CUB-200-2011](https://www.vision.caltech.edu/datasets/cub_200_2011/)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# === KONFIGURACJA ===\nDATA_DIR = './data'\nOUTPUT_DIR = './output'\nIMAGE_SIZE = 224\nBATCH_SIZE = 32\nNUM_EPOCHS = 20\nFREEZE_EPOCHS = 5\nLEARNING_RATE = 0.001\nFINE_TUNE_LR = 0.0001\nPATIENCE = 5\nNUM_WORKERS = 2\n\nos.makedirs(OUTPUT_DIR, exist_ok=True)\nprint('Konfiguracja gotowa!')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# === DANE DEMO (usuÅ„ gdy masz prawdziwe dane!) ===\nimport random\n\nBIRD_SPECIES = [\n    'American_Robin', 'Blue_Jay', 'Cardinal', 'Chickadee', 'Crow',\n    'Eagle', 'Falcon', 'Goldfinch', 'Hawk', 'Heron',\n    'Hummingbird', 'Kingfisher', 'Magpie', 'Nightingale', 'Oriole',\n    'Owl', 'Parrot', 'Pelican', 'Penguin', 'Robin',\n    'Sparrow', 'Starling', 'Swan', 'Woodpecker', 'Wren'\n]\n\ndef create_demo_data(data_dir, species_list, images_per_class=20):\n    splits = {'train': int(images_per_class * 0.7),\n              'val': max(2, int(images_per_class * 0.15)),\n              'test': max(2, int(images_per_class * 0.15))}\n    for split, count in splits.items():\n        for species in species_list:\n            d = os.path.join(data_dir, split, species)\n            os.makedirs(d, exist_ok=True)\n            br, bg, bb = hash(species)%200+30, hash(species+'g')%200+30, hash(species+'b')%200+30\n            for i in range(count):\n                arr = np.random.normal(loc=[br,bg,bb], scale=[30,30,30], size=(256,256,3)).clip(0,255).astype(np.uint8)\n                Image.fromarray(arr).save(os.path.join(d, f'{species}_{i:04d}.jpg'))\n        print(f'  {split}: {count} x {len(species_list)} = {count*len(species_list)} obrazÃ³w')\n\nif not os.path.exists(os.path.join(DATA_DIR, 'train')):\n    print('Generowanie danych demo...')\n    create_demo_data(DATA_DIR, BIRD_SPECIES)\n    print('Gotowe! (ZastÄ…p prawdziwymi zdjÄ™ciami)')\nelse:\n    print('Dane istniejÄ…!')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Augmentacja i transformacje\n\n**Augmentacja** sztucznie zwiÄ™ksza rÃ³Å¼norodnoÅ›Ä‡ danych treningowych. Stosujemy jÄ… TYLKO na danych treningowych! Normalizacja ImageNet jest kluczowa przy transfer learningu."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# === TRANSFORMACJE ===\nIMAGENET_MEAN = [0.485, 0.456, 0.406]\nIMAGENET_STD = [0.229, 0.224, 0.225]\n\ndata_transforms = {\n    'train': transforms.Compose([\n        transforms.RandomResizedCrop(IMAGE_SIZE, scale=(0.8, 1.0)),\n        transforms.RandomHorizontalFlip(0.5),\n        transforms.RandomRotation(15),\n        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n        transforms.ToTensor(),\n        transforms.Normalize(IMAGENET_MEAN, IMAGENET_STD)\n    ]),\n    'val': transforms.Compose([\n        transforms.Resize(IMAGE_SIZE + 32), transforms.CenterCrop(IMAGE_SIZE),\n        transforms.ToTensor(), transforms.Normalize(IMAGENET_MEAN, IMAGENET_STD)\n    ]),\n    'test': transforms.Compose([\n        transforms.Resize(IMAGE_SIZE + 32), transforms.CenterCrop(IMAGE_SIZE),\n        transforms.ToTensor(), transforms.Normalize(IMAGENET_MEAN, IMAGENET_STD)\n    ]),\n}\nprint('Transformacje gotowe!')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# === ÅADOWANIE DANYCH ===\nimage_datasets, dataloaders, dataset_sizes = {}, {}, {}\nfor split in ['train', 'val', 'test']:\n    d = os.path.join(DATA_DIR, split)\n    if os.path.exists(d):\n        image_datasets[split] = datasets.ImageFolder(d, transform=data_transforms[split])\n        dataloaders[split] = DataLoader(image_datasets[split], batch_size=BATCH_SIZE,\n                                         shuffle=(split=='train'), num_workers=NUM_WORKERS, pin_memory=True)\n        dataset_sizes[split] = len(image_datasets[split])\n        print(f'{split}: {dataset_sizes[split]} obrazÃ³w')\n\nclass_names = image_datasets['train'].classes\nnum_classes = len(class_names)\nprint(f'\\nGatunki ({num_classes}): {class_names[:5]}...')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# === WIZUALIZACJA PRÃ“BKI ===\ninputs, labels = next(iter(dataloaders['train']))\nfig, axes = plt.subplots(2, 4, figsize=(16, 7))\nfor i, ax in enumerate(axes.flatten()):\n    if i >= len(inputs): break\n    img = inputs[i].numpy().transpose((1,2,0))\n    img = np.array(IMAGENET_STD) * img + np.array(IMAGENET_MEAN)\n    ax.imshow(np.clip(img, 0, 1)); ax.set_title(class_names[labels[i]], fontsize=10); ax.axis('off')\nplt.suptitle('PrÃ³bka danych treningowych', fontsize=14, fontweight='bold')\nplt.tight_layout(); plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Budowa modelu\n\n### Dlaczego EfficientNet-B0?\n| Model | Parametry | ImageNet Top-1 |\n|-------|-----------|----------------|\n| ResNet-50 | 25.6M | 76.1% |\n| **EfficientNet-B0** | **5.3M** | **77.1%** |\n| VGG-16 | 138M | 71.6% |\n\nNajlepszy stosunek jakoÅ›ci do rozmiaru!\n\n### Architektura\n```\nEfficientNet-B0 (zamroÅ¼ony) â†’ Dropout â†’ Linear(1280â†’512) â†’ ReLU â†’ Dropout â†’ Linear(512â†’25)\n```"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# === BUDOWA MODELU ===\nweights = models.EfficientNet_B0_Weights.IMAGENET1K_V1\nmodel = models.efficientnet_b0(weights=weights)\nprint('ZaÅ‚adowano wagi ImageNet')\n\n# ZamraÅ¼amy warstwy bazowe\nfor param in model.parameters():\n    param.requires_grad = False\n\n# Nowa warstwa klasyfikacyjna\nnum_features = model.classifier[1].in_features\nmodel.classifier = nn.Sequential(\n    nn.Dropout(p=0.3), nn.Linear(num_features, 512),\n    nn.ReLU(), nn.Dropout(p=0.2), nn.Linear(512, num_classes))\n\nmodel = model.to(device)\ntotal = sum(p.numel() for p in model.parameters())\ntrainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\nprint(f'Parametry: {total:,} Å‚Ä…cznie, {trainable:,} trenowalnych ({trainable/total*100:.1f}%)')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Faza 1: Feature Extraction\n\nWarstwy bazowe **zamroÅ¼one**. Trenujemy TYLKO nowÄ… warstwÄ™ klasyfikacyjnÄ…. Model uczy siÄ™ mapowaÄ‡ cechy z EfficientNet na nasze gatunki."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# === FUNKCJE TRENINGOWE ===\nclass EarlyStopping:\n    def __init__(self, patience=5, min_delta=0.001):\n        self.patience, self.min_delta = patience, min_delta\n        self.counter, self.best_score, self.should_stop = 0, None, False\n    def __call__(self, val_score):\n        if self.best_score is None: self.best_score = val_score\n        elif val_score < self.best_score + self.min_delta:\n            self.counter += 1\n            if self.counter >= self.patience: self.should_stop = True; print('  Early Stopping!')\n        else: self.best_score = val_score; self.counter = 0\n\ndef train_epoch(model, dl, criterion, opt, device, size):\n    model.train(); loss_sum, correct = 0.0, 0\n    for x, y in dl:\n        x, y = x.to(device), y.to(device)\n        opt.zero_grad(); out = model(x); loss = criterion(out, y)\n        _, pred = torch.max(out, 1); loss.backward(); opt.step()\n        loss_sum += loss.item()*x.size(0); correct += torch.sum(pred == y.data)\n    return loss_sum/size, (correct.double()/size).item()\n\ndef val_epoch(model, dl, criterion, device, size):\n    model.eval(); loss_sum, correct = 0.0, 0\n    with torch.no_grad():\n        for x, y in dl:\n            x, y = x.to(device), y.to(device)\n            out = model(x); loss = criterion(out, y); _, pred = torch.max(out, 1)\n            loss_sum += loss.item()*x.size(0); correct += torch.sum(pred == y.data)\n    return loss_sum/size, (correct.double()/size).item()\n\nprint('Funkcje treningowe gotowe!')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# === FAZA 1 ===\nprint('='*50); print('FAZA 1: Feature Extraction'); print('='*50)\n\ncriterion = nn.CrossEntropyLoss()\nopt1 = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=LEARNING_RATE)\nsch1 = optim.lr_scheduler.ReduceLROnPlateau(opt1, mode='max', factor=0.5, patience=2)\nes = EarlyStopping(patience=PATIENCE)\nh1 = defaultdict(list); best_acc = 0.0; best_wts = copy.deepcopy(model.state_dict())\n\nfor epoch in range(FREEZE_EPOCHS):\n    t = time.time()\n    tl, ta = train_epoch(model, dataloaders['train'], criterion, opt1, device, dataset_sizes['train'])\n    vl, va = val_epoch(model, dataloaders['val'], criterion, device, dataset_sizes['val'])\n    print(f'Epoka {epoch+1}/{FREEZE_EPOCHS} [{time.time()-t:.1f}s] Train: {ta:.4f} | Val: {va:.4f}')\n    h1['train_loss'].append(tl); h1['train_acc'].append(ta); h1['val_loss'].append(vl); h1['val_acc'].append(va)\n    sch1.step(va)\n    if va > best_acc: best_acc = va; best_wts = copy.deepcopy(model.state_dict()); print(f'  Najlepszy: {best_acc:.4f}')\n    es(va)\n    if es.should_stop: break\n\nmodel.load_state_dict(best_wts)\nprint(f'\\nFaza 1 zakoÅ„czona! Best: {best_acc:.4f}')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Faza 2: Fine-Tuning\n\nOdmraÅ¼amy ostatnie warstwy i trenujemy z **niÅ¼szym LR**. Warstwy bazowe: 10x niÅ¼szy LR (delikatna korekta)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# === FAZA 2 ===\nprint('='*50); print('FAZA 2: Fine-Tuning'); print('='*50)\n\nfor layer in list(model.features.children())[-3:]:\n    for param in layer.parameters(): param.requires_grad = True\n\ntrainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\ntotal = sum(p.numel() for p in model.parameters())\nprint(f'Trenowalnych: {trainable:,} / {total:,} ({trainable/total*100:.1f}%)')\n\nopt2 = optim.Adam([\n    {'params': model.features.parameters(), 'lr': FINE_TUNE_LR * 0.1},\n    {'params': model.classifier.parameters(), 'lr': FINE_TUNE_LR}])\nsch2 = optim.lr_scheduler.ReduceLROnPlateau(opt2, mode='max', factor=0.5, patience=2)\nes = EarlyStopping(patience=PATIENCE)\nh2 = defaultdict(list); ft_epochs = NUM_EPOCHS - FREEZE_EPOCHS\n\nfor epoch in range(ft_epochs):\n    t = time.time()\n    tl, ta = train_epoch(model, dataloaders['train'], criterion, opt2, device, dataset_sizes['train'])\n    vl, va = val_epoch(model, dataloaders['val'], criterion, device, dataset_sizes['val'])\n    print(f'Epoka {epoch+1}/{ft_epochs} [{time.time()-t:.1f}s] Train: {ta:.4f} | Val: {va:.4f}')\n    h2['train_loss'].append(tl); h2['train_acc'].append(ta); h2['val_loss'].append(vl); h2['val_acc'].append(va)\n    sch2.step(va)\n    if va > best_acc: best_acc = va; best_wts = copy.deepcopy(model.state_dict()); print(f'  Najlepszy: {best_acc:.4f}')\n    es(va)\n    if es.should_stop: break\n\nmodel.load_state_dict(best_wts)\nprint(f'\\nFine-tuning zakoÅ„czony! Best: {best_acc:.4f}')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# === WYKRES HISTORII ===\ntl = h1['train_loss']+h2['train_loss']; vl = h1['val_loss']+h2['val_loss']\nta = h1['train_acc']+h2['train_acc']; va = h1['val_acc']+h2['val_acc']\neps = range(1, len(tl)+1); p1 = len(h1['train_loss'])\n\nfig, (a1, a2) = plt.subplots(1, 2, figsize=(14, 5))\na1.plot(eps, tl, 'b-', label='Train', lw=2); a1.plot(eps, vl, 'r-', label='Val', lw=2)\nif p1<len(eps): a1.axvline(x=p1, color='gray', ls='--', alpha=0.7, label='Fine-tuning')\na1.set_title('Loss'); a1.set_xlabel('Epoka'); a1.legend(); a1.grid(True, alpha=0.3)\n\na2.plot(eps, ta, 'b-', label='Train', lw=2); a2.plot(eps, va, 'r-', label='Val', lw=2)\nif p1<len(eps): a2.axvline(x=p1, color='gray', ls='--', alpha=0.7, label='Fine-tuning')\na2.set_title('Accuracy'); a2.set_xlabel('Epoka'); a2.legend(); a2.grid(True, alpha=0.3)\nplt.tight_layout(); plt.savefig(os.path.join(OUTPUT_DIR, 'training_history.png'), dpi=150); plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Ewaluacja modelu\n\nMierzymy: **Accuracy**, **Precision**, **Recall**, **F1-Score** i tworzymy **macierz pomyÅ‚ek**."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# === EWALUACJA ===\nmodel.eval(); all_preds, all_labels = [], []\nwith torch.no_grad():\n    for x, y in dataloaders['test']:\n        x = x.to(device); out = model(x); _, pred = torch.max(out, 1)\n        all_preds.extend(pred.cpu().numpy()); all_labels.extend(y.numpy())\n\nall_preds, all_labels = np.array(all_preds), np.array(all_labels)\ntest_acc = np.mean(all_preds == all_labels)\nprint(f'Test Accuracy: {test_acc:.4f} ({test_acc*100:.1f}%)\\n')\n\nprint(f'{\"Gatunek\":<25} {\"Prec\":>8} {\"Recall\":>8} {\"F1\":>8}')\nprint('-'*51)\nfor i, name in enumerate(class_names):\n    tp = np.sum((all_preds==i) & (all_labels==i))\n    fp = np.sum((all_preds==i) & (all_labels!=i))\n    fn = np.sum((all_preds!=i) & (all_labels==i))\n    p = tp/(tp+fp) if (tp+fp)>0 else 0; r = tp/(tp+fn) if (tp+fn)>0 else 0\n    f1 = 2*p*r/(p+r) if (p+r)>0 else 0\n    print(f'  {name:<23} {p:>8.3f} {r:>8.3f} {f1:>8.3f}')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# === CONFUSION MATRIX ===\ncm = np.zeros((num_classes, num_classes), dtype=int)\nfor t, p in zip(all_labels, all_preds): cm[t][p] += 1\n\nfig, ax = plt.subplots(figsize=(max(12, num_classes*0.5), max(10, num_classes*0.4)))\nim = ax.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\nax.figure.colorbar(im, ax=ax)\nax.set(xticks=np.arange(num_classes), yticks=np.arange(num_classes),\n       xticklabels=class_names, yticklabels=class_names,\n       title='Macierz pomyÅ‚ek', ylabel='Prawdziwa', xlabel='Przewidziana')\nplt.setp(ax.get_xticklabels(), rotation=45, ha='right')\nthresh = cm.max()/2\nfor i in range(num_classes):\n    for j in range(num_classes):\n        ax.text(j, i, str(cm[i,j]), ha='center', va='center',\n                color='white' if cm[i,j]>thresh else 'black', fontsize=7)\nplt.tight_layout(); plt.savefig(os.path.join(OUTPUT_DIR, 'confusion_matrix.png'), dpi=150); plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Predykcja na nowych obrazach\n\nFunkcja `predict_bird()` klasyfikuje nowe zdjÄ™cia ptakÃ³w i zwraca Top-5 predykcji z pewnoÅ›ciÄ…."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# === PREDYKCJA ===\ndef predict_bird(image_path, model, class_names, transform, device):\n    model.eval()\n    img = Image.open(image_path).convert('RGB')\n    inp = transform(img).unsqueeze(0).to(device)\n    with torch.no_grad():\n        out = model(inp); probs = torch.nn.functional.softmax(out, dim=1)\n        top5p, top5i = torch.topk(probs, min(5, len(class_names)))\n    \n    print(f'\\nPredykcja dla: {image_path}'); print('-'*40)\n    for i in range(top5p.size(1)):\n        n = class_names[top5i[0][i].item()]; c = top5p[0][i].item()\n        print(f'  {n:<20} {c:.1%} {chr(9608)*int(c*30)}')\n    \n    fig, (a1, a2) = plt.subplots(1, 2, figsize=(10, 4))\n    a1.imshow(img); a1.set_title(f'Predykcja: {class_names[top5i[0][0].item()]}'); a1.axis('off')\n    names = [class_names[top5i[0][i].item()] for i in range(top5p.size(1))]\n    confs = [top5p[0][i].item() for i in range(top5p.size(1))]\n    a2.barh(names[::-1], confs[::-1], color='steelblue'); a2.set_xlim(0,1); a2.set_title('Top 5')\n    plt.tight_layout(); plt.show()\n\n# PrzykÅ‚ad (odkomentuj):\n# predict_bird('Å›cieÅ¼ka/do/zdjÄ™cia.jpg', model, class_names, data_transforms['test'], device)\nprint('Funkcja predict_bird() gotowa!')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Zapis modelu i podsumowanie"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# === ZAPIS ===\nmodel_path = os.path.join(OUTPUT_DIR, 'bird_classifier.pth')\ntorch.save({'model_state_dict': model.state_dict(), 'class_names': class_names,\n            'num_classes': num_classes, 'test_accuracy': test_acc}, model_path)\nprint(f'Model zapisany: {model_path}')\n\nwith open(os.path.join(OUTPUT_DIR, 'metadata.json'), 'w') as f:\n    json.dump({'class_names': class_names, 'num_classes': num_classes,\n               'test_accuracy': float(test_acc)}, f, indent=2)\nprint('Metadane zapisane!')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# === ÅADOWANIE ZAPISANEGO MODELU ===\ndef load_saved_model(path, device):\n    ckpt = torch.load(path, map_location=device)\n    m = models.efficientnet_b0(weights=None)\n    nf = m.classifier[1].in_features\n    m.classifier = nn.Sequential(\n        nn.Dropout(0.3), nn.Linear(nf, 512), nn.ReLU(),\n        nn.Dropout(0.2), nn.Linear(512, ckpt['num_classes']))\n    m.load_state_dict(ckpt['model_state_dict'])\n    m = m.to(device); m.eval()\n    return m, ckpt['class_names']\n\n# model, classes = load_saved_model('output/bird_classifier.pth', device)\nprint('Gotowe!')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Podsumowanie\n\n### Co zrobiliÅ›my:\n1. ZaÅ‚adowaliÅ›my pretrenowany EfficientNet-B0\n2. Feature Extraction â€” trening nowej warstwy klasyfikacyjnej\n3. Fine-Tuning â€” delikatne dostrojenie caÅ‚ego modelu\n4. Ewaluacja na zbiorze testowym\n5. Funkcja predykcji dla nowych obrazÃ³w\n\n### Dalsze kroki:\n- **Prawdziwe dane**: Kaggle Birds 525 Species\n- **WiÄ™cej augmentacji**: Mixup, CutMix, RandomErasing\n- **WiÄ™kszy model**: EfficientNet-B3/B4\n- **TTA**: Test Time Augmentation\n- **Interfejs**: Gradio lub Streamlit\n\n### Przydatne linki:\n- [PyTorch Transfer Learning Tutorial](https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html)\n- [EfficientNet Paper](https://arxiv.org/abs/1905.11946)\n- [Kaggle Birds Dataset](https://www.kaggle.com/datasets/gpiosenka/100-bird-species)"
   ]
  }
 ]
}